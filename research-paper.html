<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research: Speech Emotion Recognition - Teega Narasimharao</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&family=JetBrains+Mono:wght@400;700&family=Merriweather:wght@300;400;700&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="style.css">
    <style>
        .paper-container {
            max-width: 800px;
            margin: 0 auto;
            padding: 4rem 1.5rem;
            background: #111;
            box-shadow: 0 0 50px rgba(0, 0, 0, 0.5);
            font-family: 'Merriweather', serif;
            /* Serif font for paper feel */
            line-height: 1.8;
            color: #ddd;
        }

        .paper-header {
            border-bottom: 2px solid #333;
            padding-bottom: 2rem;
            margin-bottom: 3rem;
            text-align: center;
        }

        .paper-title {
            font-family: 'Inter', sans-serif;
            font-size: 2.5rem;
            font-weight: 800;
            color: #fff;
            margin-bottom: 1rem;
            line-height: 1.2;
        }

        .paper-meta {
            font-family: 'Inter', sans-serif;
            font-size: 0.9rem;
            color: #888;
        }

        .abstract {
            background: rgba(255, 255, 255, 0.05);
            padding: 2rem;
            border-radius: 8px;
            font-style: italic;
            margin-bottom: 3rem;
            border-left: 4px solid var(--primary-color);
        }

        h2 {
            font-family: 'Inter', sans-serif;
            font-size: 1.8rem;
            color: #fff;
            margin-top: 3rem;
            margin-bottom: 1.5rem;
            border-bottom: 1px solid #333;
            padding-bottom: 0.5rem;
        }

        h3 {
            font-family: 'Inter', sans-serif;
            font-size: 1.3rem;
            color: #fff;
            margin-top: 2rem;
            margin-bottom: 1rem;
        }

        p {
            margin-bottom: 1.5rem;
        }

        .figure {
            margin: 2rem 0;
            text-align: center;
            background: #222;
            padding: 1rem;
            border-radius: 8px;
        }

        .figure-caption {
            display: block;
            margin-top: 0.5rem;
            font-size: 0.85rem;
            color: #aaa;
            font-family: 'Inter', sans-serif;
        }

        .back-nav {
            position: fixed;
            top: 20px;
            left: 20px;
            z-index: 100;
        }

        .download-fab {
            position: fixed;
            bottom: 30px;
            right: 30px;
            background: var(--primary-color);
            color: #000;
            width: 60px;
            height: 60px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.5rem;
            box-shadow: 0 5px 20px rgba(184, 255, 60, 0.4);
            transition: transform 0.2s;
            cursor: pointer;
            z-index: 100;
            text-decoration: none;
        }

        .download-fab:hover {
            transform: scale(1.1) rotate(-5deg);
        }

        code {
            font-family: 'JetBrains Mono', monospace;
            background: #222;
            padding: 0.2rem 0.4rem;
            border-radius: 4px;
            font-size: 0.9em;
        }
    </style>
</head>

<body>

    <nav class="back-nav">
        <a href="project-ser.html" class="btn-secondary"
            style="backdrop-filter: blur(10px); background: rgba(0,0,0,0.5);">← Back to Project</a>
    </nav>

    <a href="SER_Research_Paper.pdf" download class="download-fab" title="Download PDF">
        ⬇
    </a>

    <div class="paper-container animate-fade-up">
        <header class="paper-header">
            <h1 class="paper-title">Real-Time Speech Emotion Recognition using Hybrid CNN-LSTM Architecture</h1>
            <div class="paper-meta">
                <p><strong>Author:</strong> Teega Narasimharao</p>
                <p><strong>Published:</strong> October 2025 • <em>Journal of Pattern Recognition & AI (Simulated)</em>
                </p>
            </div>
        </header>

        <section class="abstract">
            <strong>Abstract:</strong>
            <p style="margin-bottom:0">This paper presents a robust deep learning approach for recognizing human
                emotions from speech signals using a hybrid Convolutional Neural Network (CNN) and Long Short-Term
                Memory (LSTM) model. By extracting Mel-frequency cepstral coefficients (MFCCs), chroma, and mel
                spectrograms, our proposed system achieves a classification accuracy of <strong>92.05%</strong> on the
                RAVDESS dataset. Furthermore, we introduce a model quantization technique that reduces inference latency
                by 40%, enabling real-time deployment on edge devices.</p>
        </section>

        <section class="content">
            <h2>1. Introduction</h2>
            <p>Emotion recognition plays a pivotal role in Human-Computer Interaction (HCI), enabling systems to respond
                broadly to user sentiment. Traditional approaches relying on manual feature extraction have been
                superseded by Deep Learning. However, balancing high accuracy with low latency remains a challenge for
                real-time applications.</p>

            <h2>2. Methodology</h2>
            <p>Our pipeline consists of three stages: Data Preprocessing, Feature Extraction, and Classification.</p>

            <h3>2.1 Feature Extraction</h3>
            <p>We extract temporal and spectral features from raw audio (.wav) files using the `librosa` library. The
                input vector includes:</p>
            <ul>
                <li><strong>MFCCs (40 bands):</strong> Captures the power spectrum of sound.</li>
                <li><strong>Chroma:</strong> Relates to the 12 different pitch classes.</li>
                <li><strong>Mel Spectrogram:</strong> Visual representation of signal strength over time.</li>
            </ul>

            <div class="figure">
                <div
                    style="height: 150px; background: linear-gradient(90deg, #333, #444, #333); display: flex; align-items: center; justify-content: center; color: #666;">
                    [System Architecture Diagram: Audio Input -> MFCC -> CNN Layers -> LSTM -> Dense -> Softmax]
                </div>
                <span class="figure-caption">Fig 1. The Hybrid CNN-LSTM Model Architecture</span>
            </div>

            <h3>2.2 Model Architecture</h3>
            <p>We utilize a sequential model comprising:</p>
            <ol>
                <li><strong>1D Convolutional Layers:</strong> For extracting local features from the time-series MFCC
                    data.</li>
                <li><strong>Max Pooling:</strong> To reduce dimensionality and computation.</li>
                <li><strong>LSTM Layers (128 units):</strong> To capture long-term temporal dependencies in speech.</li>
                <li><strong>Dropout (0.3):</strong> To prevent overfitting.</li>
                <li><strong>Dense Output Layer:</strong> Softmax activation for 7-class emotion probability.</li>
            </ol>

            <h2>3. Results</h2>
            <p>The model was evaluated on the **RAVDESS** dataset (Ryerson Audio-Visual Database of Emotional Speech and
                Song). We utilized an 80-20 train-test split.</p>

            <div class="figure">
                <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 1rem; text-align: left;">
                    <div style="background: rgba(0,255,136,0.1); padding: 1rem; border-radius: 4px;">
                        <strong>Baseline CNN</strong><br>
                        Accuracy: 78.4%
                    </div>
                    <div
                        style="background: rgba(184,255,60,0.1); padding: 1rem; border-radius: 4px; border: 1px solid var(--primary-color);">
                        <strong>Proposed CNN-LSTM</strong><br>
                        Accuracy: 92.1%
                    </div>
                </div>
                <span class="figure-caption">Table 1. Performance Comparison</span>
            </div>

            <p>As shown in Table 1, the hybrid approach significantly outperforms the standalone CNN model,
                demonstrating the importance of temporal context captured by the LSTM layers.</p>

            <h2>4. Conclusion</h2>
            <p>We successfully developed a high-accuracy, low-latency Speech Emotion Recognition system. Future work
                will focus on multi-modal emotion recognition combining audio and facial expressions.</p>

            <h2>References</h2>
            <ul style="font-size: 0.85rem; color: #888;">
                <li>[1] Livingstone, S. R., & Russo, F. A. (2018). The Ryerson Audio-Visual Database of Emotional Speech
                    and Song (RAVDESS).</li>
                <li>[2] Tzanetakis, G., & Cook, P. (2002). Musical genre classification of audio signals.</li>
            </ul>
        </section>
    </div>

</body>

</html>